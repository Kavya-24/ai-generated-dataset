{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This notebook is purely for learning purpose and makes use of open sourced text-generational models to generate one-liner data using HuggingFace framework\n",
    "Reference Medium Article: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install hugging-face and all dependencies\n",
    "!pip install huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set-up hugging face on Kaggle: https://medium.com/@kavya24goyal/setting-up-hugging-face-on-kaggle-notebooks-486e85f91cab)\n",
    "# set-up hugging face on Google Colab: https://medium.com/@kavya24goyal/setting-up-hugging-face-on-google-colaboratory-49b60fc60bd0 \n",
    "from huggingface_hub import notebook_login\n",
    "\n",
    "# this steps requires your HuggingFace API Key\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a pipeline as a high-level helper and load Phi-3 model\n",
    "# model documentation: https://huggingface.co/microsoft/Phi-3-mini-4k-instruct\n",
    "from transformers import pipeline\n",
    "pipe = pipeline(\"text-generation\", model=\"microsoft/Phi-3-mini-4k-instruct\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "   {\"role\": \"system\", \"content\": \"You are a helpful AI assistant of a software company who creates one-liner operational requests\"}, \n",
    "   {\"role\": \"user\", \"content\": \"Generate an operational request in 10-25 words\"},\n",
    "]\n",
    "\n",
    "\n",
    "generation_args = { \n",
    "    # how many tokens you want in your output. \n",
    "    # one word is approximately 4/3 tokens\n",
    "    \"max_new_tokens\": 50, \n",
    "    # how much randomness you want in your result OR\n",
    "    # by what extent you want the model to be creative on its own\n",
    "    # I am keeping it high because I do not have a source data to work on\n",
    "    # and I want the data to be creative\n",
    "    \"temperature\": 0.8, \n",
    "    \"do_sample\": True\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "\n",
    "# clean up the response from the assistant for dataset\n",
    "def sanitize_assistant_response(prompt_result):\n",
    "    content = [item['content'] for item in prompt_result[0]['generated_text'] if item['role'] == 'assistant'][0]\n",
    "    return content.strip().replace('\"', '')\n",
    "\n",
    "# loop and generate 1000s of requests\n",
    "def ai_generated_requests(batch_size: int) -> list:\n",
    "    requests = []\n",
    "    start_time = time.time()\n",
    "    for i in range(batch_size):\n",
    "        generation_args['temperature'] = random.uniform(0.7, 1)\n",
    "        prompt_result = pipe(messages, **generation_args)\n",
    "        requests.append(sanitize_assistant_response(prompt_result))\n",
    "        print(f'generated request {i+1}')\n",
    "    return requests, time.time() - start_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate dataset. All your requests\n",
    "requests, time_taken = ai_generated_requests(1000)\n",
    "print(f'total time taken={time_taken}')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
